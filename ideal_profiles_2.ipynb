{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re, pandas as pd\n",
    "from selenium import webdriver\n",
    "#import sys, os\n",
    "import json\n",
    "#import time\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    Given the url of a page, this function returns the soup object.\n",
    "    \n",
    "    Arguments:\n",
    "    url -- the link to get soup object for\n",
    "    \n",
    "    Returns:\n",
    "    soup - soup object\n",
    "    \"\"\"\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.close()\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_job_links(soup):\n",
    "    \"\"\"\n",
    "    Grab all non-sponsored job posting links from a Indeed search result page using the given soup object\n",
    "    \n",
    "    Arguments:\n",
    "    soup -- the soup object corresponding to a search result page\n",
    "            e.g. https://ca.indeed.com/jobs?q=data+scientist&l=Toronto&start=20\n",
    "    \n",
    "    Returns:\n",
    "    urls -- a python list of job posting urls\n",
    "    \n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    \n",
    "    # Loop thru all the posting links\n",
    "    for link in soup.find_all('h2', {'class': 'jobtitle'}):\n",
    "        # Since sponsored job postings are represented by \"a target\" instead of \"a href\", no need to worry here\n",
    "        partial_url = link.a.get('href')\n",
    "        # This is a partial url, we need to attach the prefix\n",
    "        url = 'https://ca.indeed.com' + partial_url\n",
    "        # Make sure this is not a sponsored posting\n",
    "        urls.append(url)\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(query, num_pages, location):\n",
    "    \"\"\"\n",
    "    Get all the job posting URLs resulted from a specific search.\n",
    "    \n",
    "    Arguments:\n",
    "    query -- job title to query\n",
    "    num_pages -- number of pages needed\n",
    "    location -- city to search in\n",
    "    \n",
    "    Returns:\n",
    "    urls -- a list of job posting URL's (when num_pages valid)\n",
    "    max_pages -- maximum number of pages allowed ((when num_pages invalid))\n",
    "    \"\"\"\n",
    "    # We always need the first page\n",
    "    base_url = 'https://ca.indeed.com/jobs?q={}&l={}'.format(query, location)\n",
    "    soup = get_soup(base_url)\n",
    "    urls = grab_job_links(soup)\n",
    "    \n",
    "    # Get the total number of postings found \n",
    "    posting_count_string = soup.find(name='div', attrs={'id':\"searchCount\"}).get_text()\n",
    "    posting_count_string = posting_count_string[posting_count_string.find('of')+2:].strip()\n",
    "    #print('posting_count_string: {}'.format(posting_count_string))\n",
    "    #print('type is: {}'.format(type(posting_count_string)))\n",
    "    \n",
    "    try:\n",
    "        posting_count = int(posting_count_string)\n",
    "    except ValueError: # deal with special case when parsed string is \"360 jobs\"\n",
    "        posting_count = int(re.search('\\d+', posting_count_string).group(0))\n",
    "        #print('posting_count: {}'.format(posting_count))\n",
    "        #print('\\ntype: {}'.format(type(posting_count)))\n",
    "    finally:\n",
    "        posting_count = 330 # setting to 330 when unable to get the total\n",
    "        pass\n",
    "    \n",
    "    # Limit nunmber of pages to get\n",
    "    max_pages = round(posting_count / 10) - 3\n",
    "    if num_pages > max_pages:\n",
    "        print('returning max_pages!!')\n",
    "        return max_pages\n",
    "    \n",
    "        # Additional work is needed when more than 1 page is requested\n",
    "    if num_pages >= 2:\n",
    "        # Start loop from page 2 since page 1 has been dealt with above\n",
    "        for i in range(2, num_pages+1):\n",
    "            num = (i-1) * 10\n",
    "            base_url = 'https://ca.indeed.com/jobs?q={}&l={}&start={}'.format(query, location, num)\n",
    "            try:\n",
    "                soup = get_soup(base_url)\n",
    "                # We always combine the results back to the list\n",
    "                urls += grab_job_links(soup)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Check to ensure the number of urls gotten is correct\n",
    "    #assert len(urls) == num_pages * 10, \"There are missing job links, check code!\"\n",
    "\n",
    "    return urls     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posting(url):\n",
    "    \"\"\"\n",
    "    Get the text portion including both title and job description of the job posting from a given url\n",
    "    \n",
    "    Arguments:\n",
    "    url -- The job posting link\n",
    "        \n",
    "    Returns:\n",
    "    title -- the job title (if \"data scientist\" is in the title)\n",
    "    posting -- the job posting content    \n",
    "    \"\"\"\n",
    "    # Get the url content as BS object\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # The job title is held in the h3 tag\n",
    "    title = soup.find(name='h3').getText().lower()\n",
    "    posting = soup.find(name='div', attrs={'class': \"jobsearch-JobComponent\"}).get_text()\n",
    "\n",
    "    return title, posting.lower()\n",
    "\n",
    "        \n",
    "    #if 'data scientist' in title:  # We'll proceed to grab the job posting text if the title is correct\n",
    "        # All the text info is contained in the div element with the below class, extract the text.\n",
    "        #posting = soup.find(name='div', attrs={'class': \"jobsearch-JobComponent\"}).get_text()\n",
    "        #return title, posting.lower()\n",
    "    #else:\n",
    "        #return False\n",
    "    \n",
    "        # Get rid of numbers and symbols other than given\n",
    "        #text = re.sub(\"[^a-zA-Z'+#&]\", \" \", text)\n",
    "        # Convert to lower case and split to list and then set\n",
    "        #text = text.lower().strip()\n",
    "    \n",
    "        #return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(query, num_pages, location='Toronto'):\n",
    "    \"\"\"\n",
    "    Get all the job posting data and save in a json file using below structure:\n",
    "    \n",
    "    {<count>: {'title': ..., 'posting':..., 'url':...}...}\n",
    "    \n",
    "    The json file name has this format: \"\"<query>.json\"\n",
    "    \n",
    "    Arguments:\n",
    "    query -- Indeed query keyword such as 'Data Scientist'\n",
    "    num_pages - Number of search results needed\n",
    "    location -- location to search for\n",
    "    \n",
    "    Returns:\n",
    "    postings_dict -- Python dict including all posting data\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert the queried title to Indeed format\n",
    "    query = '+'.join(query.lower().split())\n",
    "    \n",
    "    postings_dict = {}\n",
    "    urls = get_urls(query, num_pages, location)\n",
    "    \n",
    "    #  Continue only if the requested number of pages is valid (when invalid, a number is returned instead of list)\n",
    "    if isinstance(urls, list):\n",
    "        num_urls = len(urls)\n",
    "        for i, url in enumerate(urls):\n",
    "            try:\n",
    "                title, posting = get_posting(url)\n",
    "                postings_dict[i] = {}\n",
    "                postings_dict[i]['title'], postings_dict[i]['posting'], postings_dict[i]['url'] = \\\n",
    "                title, posting, url\n",
    "            except: \n",
    "                continue\n",
    "            \n",
    "            percent = (i+1) / num_urls\n",
    "            # Print the progress the \"end\" arg keeps the message in the same line \n",
    "            print(\"Progress: {:2.0f}%\".format(100*percent), end='\\r')\n",
    "\n",
    "        # Save the dict as json file\n",
    "        file_name = query.replace('+', '_') + '.json'\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(postings_dict, f)\n",
    "        \n",
    "        print('All {} postings have been scraped and saved!'.format(num_urls))    \n",
    "        #return postings_dict\n",
    "    else:\n",
    "        print(\"Due to similar results, maximum number of pages is only {}. Please try again!\".format(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    Open the saved json data file and load the data into a dict.\n",
    "    \n",
    "    Argument:\n",
    "    file_name -- the saved file name, e.g. \"machine_learning_engineer.json\"\n",
    "    \n",
    "    Returns:\n",
    "    postings_dict -- data in dict format   \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_name, 'r') as f:\n",
    "        postings_dict = json.load(f)\n",
    "        return postings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the text so that all words are root...\n",
    "    \n",
    "    Arguments:\n",
    "    text -- list of job posting strings\n",
    "        \n",
    "    Returns:\n",
    "    cleaned_text -- a text string for the wc plot\n",
    "    \"\"\"\n",
    "    # Split the text based on slash, space and newline, then take set     \n",
    "    #text = [set(re.split('/| |\\n|', i)) for i in text]\n",
    "    text = [set(re.split('\\W', i)) for i in text]\n",
    "    \n",
    "    cleaned_text = []\n",
    "    for i in text:\n",
    "        cleaned_text += list(i)\n",
    "    cleaned_text = ' '.join(cleaned_text)\n",
    "    \n",
    "    return cleaned_text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_list(postings_dict, first_n_postings=100):\n",
    "    \n",
    "    text = []\n",
    "    for i in range(0, first_n_postings+1):\n",
    "        text.append(postings_dict[str(i)]['posting'])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wc(text, max_words=200, stopwords_list=[], to_file_name=None):\n",
    "    \"\"\"\n",
    "    Make a word cloud plot using the given text.\n",
    "    \n",
    "    Arguments:\n",
    "    text -- the text as a string\n",
    "    \n",
    "    Returns:\n",
    "    None    \n",
    "    \"\"\"\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update(stopwords_list)\n",
    "\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "                         stopwords=stopwords,\n",
    "                         #prefer_horizontal=1,\n",
    "                         max_words=max_words, \n",
    "                         min_font_size=6,\n",
    "                         scale=1,\n",
    "                         width = 800, height = 800, \n",
    "                         random_state=8).generate(text)\n",
    "    plt.figure(figsize=[16,16])\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    if to_file_name:\n",
    "        to_file_name = to_file_name + \".png\"\n",
    "        wordcloud.to_file(to_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 290 postings have been scraped and saved!\n"
     ]
    }
   ],
   "source": [
    "get_data(query='machine learning engineer', num_pages=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('machine_learning_engineer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accommodation',\n",
       " 'ago',\n",
       " 'application',\n",
       " 'based',\n",
       " 'canada',\n",
       " 'candidate',\n",
       " 'company',\n",
       " 'data',\n",
       " 'days',\n",
       " 'education',\n",
       " 'employee',\n",
       " 'ensure',\n",
       " 'environment',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'experience',\n",
       " 'help',\n",
       " 'including',\n",
       " 'job',\n",
       " 'jobapply',\n",
       " 'life',\n",
       " 'location',\n",
       " 'microsoft',\n",
       " 'nowapply',\n",
       " 'office',\n",
       " 'preferred',\n",
       " 'people',\n",
       " 'qualifications',\n",
       " 'required',\n",
       " 'requirement',\n",
       " 'requirements',\n",
       " 'resume',\n",
       " 'review',\n",
       " 'reviews',\n",
       " 'reviewsread',\n",
       " 'role',\n",
       " 'save',\n",
       " 'saying',\n",
       " 'scientist',\n",
       " 'self',\n",
       " 'service',\n",
       " 'sitesave',\n",
       " 'skill',\n",
       " 'skills',\n",
       " 'time',\n",
       " 'tool',\n",
       " 'toronto',\n",
       " 'understanding',\n",
       " 'us',\n",
       " 'well',\n",
       " 'will',\n",
       " 'work',\n",
       " 'working',\n",
       " 'world',\n",
       " 'year',\n",
       " 'yearsjobapply']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list = ['accommodation', 'ago', 'application', 'based', 'canada', 'candidate', 'company', 'data', 'days', \n",
    "                  'education', 'employee', 'ensure', 'environment', 'et', 'etc', 'experience', 'help', 'including', \n",
    "                  'job', 'jobapply', 'life', 'location', 'microsoft', 'nowapply', 'office', 'preferred', 'people',\n",
    "                  'qualifications', 'required', 'requirement', 'requirements', 'resume', 'review','reviews',\n",
    "                  'reviewsread', 'role', 'save', 'saying', 'scientist', 'self', 'service', 'sitesave', 'skill', \n",
    "                  'skills', 'time', 'tool', 'toronto', 'understanding', 'us', 'well', 'will', 'work', 'working', \n",
    "                  'world', 'year', 'yearsjobapply']\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo's\n",
    "- stemming etc.\n",
    "- docstring and comments\n",
    "- OOP\n",
    "- add progress update text\n",
    "- single responsiblity principle for functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
